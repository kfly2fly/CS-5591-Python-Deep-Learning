{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP11.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "O6LDQMN1V55P",
        "BW2rEbZuVzWt",
        "D3ILNkvHVlsW"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ICP 11 - Sentiment Analysis\n",
        "Keenan Flynn and Jasmine Thai"
      ],
      "metadata": {
        "id": "tWNEbtqTi15S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages and dependencies"
      ],
      "metadata": {
        "id": "7FE2nDU5i8WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, GlobalMaxPool1D, MaxPooling1D, Conv1D, Flatten\n",
        "from keras.losses import BinaryCrossentropy\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "from nltk import WordNetLemmatizer  \n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qauwuDeZ40K",
        "outputId": "58ca4dcf-c6d8-483e-c04d-0afdac3324d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Processing"
      ],
      "metadata": {
        "id": "qM247g4jnU0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the reviews had commas within the text of the review.\n",
        "This read in the columns incorrectly so we added a quotechar parameter. We also dropped the unsupervised labels"
      ],
      "metadata": {
        "id": "K_qo-1IXa7WI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0M-KDAeZlv-"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('imdb_master.csv',encoding='latin-1', quotechar='\"')\n",
        "df = df[df['label'] != 'unsup']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the columns as a numpy array"
      ],
      "metadata": {
        "id": "FBbOBI3znLUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df['review'].values\n",
        "y = df['label'].values"
      ],
      "metadata": {
        "id": "hHptFhUIfx96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CAEw70UOstIN",
        "outputId": "8d457f53-1487-42d3-bc33-3675ce40cc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0   type                                             review  \\\n",
              "0               0   test  Once again Mr. Costner has dragged out a movie...   \n",
              "1               1   test  This is an example of why the majority of acti...   \n",
              "2               2   test  First of all I hate those moronic rappers, who...   \n",
              "3               3   test  Not even the Beatles could write songs everyon...   \n",
              "4               4   test  Brass pictures (movies is not a fitting word f...   \n",
              "...           ...    ...                                                ...   \n",
              "49995       49995  train  Seeing as the vote average was pretty low, and...   \n",
              "49996       49996  train  The plot had some wretched, unbelievable twist...   \n",
              "49997       49997  train  I am amazed at how this movie(and most others ...   \n",
              "49998       49998  train  A Christmas Together actually came before my t...   \n",
              "49999       49999  train  Working-class romantic drama from director Mar...   \n",
              "\n",
              "      label         file  \n",
              "0       neg      0_2.txt  \n",
              "1       neg  10000_4.txt  \n",
              "2       neg  10001_1.txt  \n",
              "3       neg  10002_3.txt  \n",
              "4       neg  10003_3.txt  \n",
              "...     ...          ...  \n",
              "49995   pos   9998_9.txt  \n",
              "49996   pos   9999_8.txt  \n",
              "49997   pos   999_10.txt  \n",
              "49998   pos     99_8.txt  \n",
              "49999   pos      9_7.txt  \n",
              "\n",
              "[50000 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ec5e08d-971d-454d-9b19-749651bbd3e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>type</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
              "      <td>neg</td>\n",
              "      <td>0_2.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10000_4.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>test</td>\n",
              "      <td>First of all I hate those moronic rappers, who...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10001_1.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10002_3.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>test</td>\n",
              "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10003_3.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>49995</td>\n",
              "      <td>train</td>\n",
              "      <td>Seeing as the vote average was pretty low, and...</td>\n",
              "      <td>pos</td>\n",
              "      <td>9998_9.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>49996</td>\n",
              "      <td>train</td>\n",
              "      <td>The plot had some wretched, unbelievable twist...</td>\n",
              "      <td>pos</td>\n",
              "      <td>9999_8.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>49997</td>\n",
              "      <td>train</td>\n",
              "      <td>I am amazed at how this movie(and most others ...</td>\n",
              "      <td>pos</td>\n",
              "      <td>999_10.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>49998</td>\n",
              "      <td>train</td>\n",
              "      <td>A Christmas Together actually came before my t...</td>\n",
              "      <td>pos</td>\n",
              "      <td>99_8.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>49999</td>\n",
              "      <td>train</td>\n",
              "      <td>Working-class romantic drama from director Mar...</td>\n",
              "      <td>pos</td>\n",
              "      <td>9_7.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ec5e08d-971d-454d-9b19-749651bbd3e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3ec5e08d-971d-454d-9b19-749651bbd3e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3ec5e08d-971d-454d-9b19-749651bbd3e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Cleaning & Pre-Processing"
      ],
      "metadata": {
        "id": "T4KRrPkIpmd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show what the sentence attribute looks like before pre-proccessing"
      ],
      "metadata": {
        "id": "5bVqKWcAnO4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRcu1DRBqjeK",
        "outputId": "55e6bcb7-37b2-48c0-ee48-e9de9195fbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\",\n",
              "       \"This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying...<br /><br />Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\",\n",
              "       \"First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against their foreheads. All they do is curse and shoot each other and acting like clichÃ©'e version of gangsters.<br /><br />The movie doesn't take more than five minutes to explain what is going on before we're already at the warehouse There is not a single sympathetic character in this movie, except for the homeless guy, who is also the only one with half a brain.<br /><br />Bill Paxton and William Sadler are both hill billies and Sadlers character is just as much a villain as the gangsters. I did'nt like him right from the start.<br /><br />The movie is filled with pointless violence and Walter Hills specialty: people falling through windows with glass flying everywhere. There is pretty much no plot and it is a big problem when you root for no-one. Everybody dies, except from Paxton and the homeless guy and everybody get what they deserve.<br /><br />The only two black people that can act is the homeless guy and the junkie but they're actors by profession, not annoying ugly brain dead rappers.<br /><br />Stay away from this crap and watch 48 hours 1 and 2 instead. At lest they have characters you care about, a sense of humor and nothing but real actors in the cast.\",\n",
              "       ...,\n",
              "       \"I am amazed at how this movie(and most others has a average 5 stars and lower when there are crappy movies averaging 7 to 10 stars on IMDb. The fanboy mentality strikes again. When this movie came out just about everyone slammed it. Even my ex-girlfriend said this movie questionable. Years later I sat down to watch this movie and I found myself enjoying. Even laughing quite a bit. This and The Replacement Killers are the movies that had people labeling the director Antoine Fuqua as the black Michael Bay. I don't see how since most of Fuqua's movies are smarter than anything Michael Bay has came up with. At any rate...<br /><br />Story: Alvin Sanders(Jamie Foxx) is former convict that is used by a no-nonsense Treasury agent Edgar(David Morse) as a pawn to catch a killer named Bristol(Doug Hutchinson). Alvin's every moves are tracked by a bug implanted in his jaw after an accident. While these agents are after Bristol, Bristol is after the gold bricks that were taken in a heist gone awry.<br /><br />Jamie Foxx is funny as well as great as Alvin Sanders. Alvin is a fast-talker that is a lot smarter than he lets on. Doug Hutchinson is okay as Bristol. He can be over-the-top sometimes in his John Malkovitchesque demeanor. He was better here than he was as Looney Bin Jim in Punisher: War Zone. David Morse is good as the hard edged treasury agent. Even Mike Epps is funny as Alvin's brother Stevie. Both him and Jamie had some funny moments on screen.<br /><br />The only flaw of the movie is the some of the attempts at a thriller fall flat. The scenario at the horse race track is way over-the-top but I couldn't look away. The director went all out there so he gets points for that. Plus the bomb scene with the treasury agent tied to a chair while the detonator rests on the door was pretty nifty.<br /><br />All in all Bait is not a bad movie by a long shot. Its never boring, its always funny and I wasn't checking my watch every minute. That should count for something. Bait is one of the most underrated movies of 2000 period.<br /><br />PS: to the reviewer that claimed this movie is too violent.... How long have you been living under a rock? I'm pretty sure you've seen the Die Hard series and EVERY movie by Quentin Tarantino. But those movies aren't violent right? Weirdo.\",\n",
              "       \"A Christmas Together actually came before my time, but I've been raised on John Denver and the songs from this special were always my family's Christmas music. For years we had a crackling cassette made from a record that meant it was Christmas. A few years ago, I was finally able to track down a video of it on Ebay, so after listening to all the music for some 21 years, I got to see John and the Muppets in action for myself. If you ever get the chance, it's a lot of fun--great music, heart-warming and cheesy. It's also interesting to see the 70's versions of the Muppets and compare them to their newer versions today. I believe Denver actually took some heat for doing a show like this--I guess normally performers don't compromise their images by doing sing-a-longs with the Muppets, but I'm glad he did. Even if you can't track down the video, the soundtrack is worth it too. It has some Muppified traditional favorites, but also some original Denver tunes as well.\",\n",
              "       'Working-class romantic drama from director Martin Ritt is as unbelievable as they come, yet there are moments of pleasure due mostly to the charisma of stars Jane Fonda and Robert De Niro (both terrific). She\\'s a widow who can\\'t move on, he\\'s illiterate and a closet-inventor--you can probably guess the rest. Adaptation of Pat Barker\\'s novel \"Union Street\" (a better title!) is so laid-back it verges on bland, and the film\\'s editing is a mess, but it\\'s still pleasant; a rosy-hued blue-collar fantasy. There are no overtures to serious issues (even the illiteracy angle is just a plot-tool for the ensuing love story) and no real fireworks, though the characters are intentionally a bit colorless and the leads are toned down to an interesting degree. The finale is pure fluff--and cynics will find it difficult to swallow--though these two characters deserve a happy ending and the picture wouldn\\'t really be satisfying any other way. *** from ****'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop removes any punctuation, changes any capitol letters to lowercase, and cleans the text so that it is a better input for the Embedding layer of the NN. We use the Regex library to do this."
      ],
      "metadata": {
        "id": "GJyrAqCSnZDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  sentences[i]=sentences[i].translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "  sentences[i]=''.join(i for i in sentences[i] if not i.isdigit())\n",
        "  sentences[i]=re.sub(r'\\s+',' ',sentences[i],flags=re.I)\n",
        "  sentences[i]=re.sub(r'[!@#$%^&*()_+|\\}{;:/><.}]','',sentences[i],flags=re.I)\n",
        "  sentences[i]=re.sub(r'\\s+[a-zA-Z]\\s+', ' ',sentences[i])\n",
        "  sentences[i]=re.sub(r\"[a-zA-Z]\", lambda x :  x.group(0).lower(), sentences[i])\n",
        "  sentences=df.iloc[:,2].values"
      ],
      "metadata": {
        "id": "vnio4KX4Rh7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the sentence attribute after Regex modifications"
      ],
      "metadata": {
        "id": "LhXrVDhdnsL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iWtz57Qq3iU",
        "outputId": "0c273ae6-c0e8-4a04-c731-416249d2c200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['once again mr costner has dragged out movie for far longer than necessary aside from the terrific sea rescue sequences of which there are very few just did not care about any of the characters most of us have ghosts in the closet and costners character are realized early on and then forgotten until much later by which time did not care the character we should really care about is very cocky overconfident ashton kutcher the problem is he comes off as kid who thinks hes better than anyone else around him and shows no signs of cluttered closet his only obstacle appears to be winning over costner finally when we are well past the half way point of this stinker costner tells us all about kutchers ghosts we are told why kutcher is driven to be the best with no prior inkling or foreshadowing no magic here it was all could do to keep from turning it off an hour in',\n",
              "       'this is an example of why the majority of action films are the same generic and boring theres really nothing worth watching here complete waste of the then barelytapped talents of icet and ice cube whove each proven many times over that they are capable of acting and acting well dont bother with this one go see new jack city ricochet or watch new york undercover for icet or boyz the hood higher learning or friday for ice cube and see the real deal icets horribly cliched dialogue alone makes this film grate at the teeth and im still wondering what the heck bill paxton was doing in this film and why the heck does he always play the exact same character from aliens onward every film ive seen with bill paxton has him playing the exact same irritating character and at least in aliens his character died which made it somewhat gratifyingbr br overall this is secondrate action trash there are countless better films to see and if you really want to see this one watch judgement night which is practically carbon copy but has better acting and better script the only thing that made this at all worth watching was decent hand on the camera the cinematography was almost refreshing which comes close to making up for the horrible film itself but not quite ',\n",
              "       'first of all hate those moronic rappers who couldnt act if they had gun pressed against their foreheads all they do is curse and shoot each other and acting like clichÃ©e version of gangstersbr br the movie doesnt take more than five minutes to explain what is going on before were already at the warehouse there is not single sympathetic character in this movie except for the homeless guy who is also the only one with half brainbr br bill paxton and william sadler are both hill billies and sadlers character is just as much villain as the gangsters didnt like him right from the startbr br the movie is filled with pointless violence and walter hills specialty people falling through windows with glass flying everywhere there is pretty much no plot and it is big problem when you root for noone everybody dies except from paxton and the homeless guy and everybody get what they deservebr br the only two black people that can act is the homeless guy and the junkie but theyre actors by profession not annoying ugly brain dead rappersbr br stay away from this crap and watch hours and instead at lest they have characters you care about sense of humor and nothing but real actors in the cast',\n",
              "       ...,\n",
              "       'i am amazed at how this movieand most others has average stars and lower when there are crappy movies averaging to stars on imdb the fanboy mentality strikes again when this movie came out just about everyone slammed it even my exgirlfriend said this movie questionable years later sat down to watch this movie and found myself enjoying even laughing quite bit this and the replacement killers are the movies that had people labeling the director antoine fuqua as the black michael bay dont see how since most of fuquas movies are smarter than anything michael bay has came up with at any ratebr br story alvin sandersjamie foxx is former convict that is used by nononsense treasury agent edgardavid morse as pawn to catch killer named bristoldoug hutchinson alvins every moves are tracked by bug implanted in his jaw after an accident while these agents are after bristol bristol is after the gold bricks that were taken in heist gone awrybr br jamie foxx is funny as well as great as alvin sanders alvin is fasttalker that is lot smarter than he lets on doug hutchinson is okay as bristol he can be overthetop sometimes in his john malkovitchesque demeanor he was better here than he was as looney bin jim in punisher war zone david morse is good as the hard edged treasury agent even mike epps is funny as alvins brother stevie both him and jamie had some funny moments on screenbr br the only flaw of the movie is the some of the attempts at thriller fall flat the scenario at the horse race track is way overthetop but couldnt look away the director went all out there so he gets points for that plus the bomb scene with the treasury agent tied to chair while the detonator rests on the door was pretty niftybr br all in all bait is not bad movie by long shot its never boring its always funny and wasnt checking my watch every minute that should count for something bait is one of the most underrated movies of periodbr br ps to the reviewer that claimed this movie is too violent how long have you been living under rock im pretty sure youve seen the die hard series and every movie by quentin tarantino but those movies arent violent right weirdo',\n",
              "       'a christmas together actually came before my time but ive been raised on john denver and the songs from this special were always my familys christmas music for years we had crackling cassette made from record that meant it was christmas few years ago was finally able to track down video of it on ebay so after listening to all the music for some years got to see john and the muppets in action for myself if you ever get the chance its lot of fungreat music heartwarming and cheesy its also interesting to see the versions of the muppets and compare them to their newer versions today believe denver actually took some heat for doing show like thisi guess normally performers dont compromise their images by doing singalongs with the muppets but im glad he did even if you cant track down the video the soundtrack is worth it too it has some muppified traditional favorites but also some original denver tunes as well',\n",
              "       'workingclass romantic drama from director martin ritt is as unbelievable as they come yet there are moments of pleasure due mostly to the charisma of stars jane fonda and robert de niro both terrific shes widow who cant move on hes illiterate and closetinventoryou can probably guess the rest adaptation of pat barkers novel union street better title is so laidback it verges on bland and the films editing is mess but its still pleasant rosyhued bluecollar fantasy there are no overtures to serious issues even the illiteracy angle is just plottool for the ensuing love story and no real fireworks though the characters are intentionally bit colorless and the leads are toned down to an interesting degree the finale is pure fluffand cynics will find it difficult to swallowthough these two characters deserve happy ending and the picture wouldnt really be satisfying any other way from '],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use nltk word_tokenize() to sepearte each review into a list of words."
      ],
      "metadata": {
        "id": "qnRKbU3HnzxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = np.array([word_tokenize(x) for x in sentences])"
      ],
      "metadata": {
        "id": "PhxSUJWdubXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e5408c-3e78-451e-e248-f9a747a31dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what the sentences attribute looks like after tokenization."
      ],
      "metadata": {
        "id": "eAIk5bdWn9Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "mbav45-cvppH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0d81bf-dd77-457f-c1d5-085959082716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['once', 'again', 'mr', 'costner', 'has', 'dragged', 'out', 'movie', 'for', 'far', 'longer', 'than', 'necessary', 'aside', 'from', 'the', 'terrific', 'sea', 'rescue', 'sequences', 'of', 'which', 'there', 'are', 'very', 'few', 'just', 'did', 'not', 'care', 'about', 'any', 'of', 'the', 'characters', 'most', 'of', 'us', 'have', 'ghosts', 'in', 'the', 'closet', 'and', 'costners', 'character', 'are', 'realized', 'early', 'on', 'and', 'then', 'forgotten', 'until', 'much', 'later', 'by', 'which', 'time', 'did', 'not', 'care', 'the', 'character', 'we', 'should', 'really', 'care', 'about', 'is', 'very', 'cocky', 'overconfident', 'ashton', 'kutcher', 'the', 'problem', 'is', 'he', 'comes', 'off', 'as', 'kid', 'who', 'thinks', 'hes', 'better', 'than', 'anyone', 'else', 'around', 'him', 'and', 'shows', 'no', 'signs', 'of', 'cluttered', 'closet', 'his', 'only', 'obstacle', 'appears', 'to', 'be', 'winning', 'over', 'costner', 'finally', 'when', 'we', 'are', 'well', 'past', 'the', 'half', 'way', 'point', 'of', 'this', 'stinker', 'costner', 'tells', 'us', 'all', 'about', 'kutchers', 'ghosts', 'we', 'are', 'told', 'why', 'kutcher', 'is', 'driven', 'to', 'be', 'the', 'best', 'with', 'no', 'prior', 'inkling', 'or', 'foreshadowing', 'no', 'magic', 'here', 'it', 'was', 'all', 'could', 'do', 'to', 'keep', 'from', 'turning', 'it', 'off', 'an', 'hour', 'in']),\n",
              "       list(['this', 'is', 'an', 'example', 'of', 'why', 'the', 'majority', 'of', 'action', 'films', 'are', 'the', 'same', 'generic', 'and', 'boring', 'theres', 'really', 'nothing', 'worth', 'watching', 'here', 'complete', 'waste', 'of', 'the', 'then', 'barelytapped', 'talents', 'of', 'icet', 'and', 'ice', 'cube', 'whove', 'each', 'proven', 'many', 'times', 'over', 'that', 'they', 'are', 'capable', 'of', 'acting', 'and', 'acting', 'well', 'dont', 'bother', 'with', 'this', 'one', 'go', 'see', 'new', 'jack', 'city', 'ricochet', 'or', 'watch', 'new', 'york', 'undercover', 'for', 'icet', 'or', 'boyz', 'the', 'hood', 'higher', 'learning', 'or', 'friday', 'for', 'ice', 'cube', 'and', 'see', 'the', 'real', 'deal', 'icets', 'horribly', 'cliched', 'dialogue', 'alone', 'makes', 'this', 'film', 'grate', 'at', 'the', 'teeth', 'and', 'im', 'still', 'wondering', 'what', 'the', 'heck', 'bill', 'paxton', 'was', 'doing', 'in', 'this', 'film', 'and', 'why', 'the', 'heck', 'does', 'he', 'always', 'play', 'the', 'exact', 'same', 'character', 'from', 'aliens', 'onward', 'every', 'film', 'ive', 'seen', 'with', 'bill', 'paxton', 'has', 'him', 'playing', 'the', 'exact', 'same', 'irritating', 'character', 'and', 'at', 'least', 'in', 'aliens', 'his', 'character', 'died', 'which', 'made', 'it', 'somewhat', 'gratifyingbr', 'br', 'overall', 'this', 'is', 'secondrate', 'action', 'trash', 'there', 'are', 'countless', 'better', 'films', 'to', 'see', 'and', 'if', 'you', 'really', 'want', 'to', 'see', 'this', 'one', 'watch', 'judgement', 'night', 'which', 'is', 'practically', 'carbon', 'copy', 'but', 'has', 'better', 'acting', 'and', 'better', 'script', 'the', 'only', 'thing', 'that', 'made', 'this', 'at', 'all', 'worth', 'watching', 'was', 'decent', 'hand', 'on', 'the', 'camera', 'the', 'cinematography', 'was', 'almost', 'refreshing', 'which', 'comes', 'close', 'to', 'making', 'up', 'for', 'the', 'horrible', 'film', 'itself', 'but', 'not', 'quite']),\n",
              "       list(['first', 'of', 'all', 'hate', 'those', 'moronic', 'rappers', 'who', 'couldnt', 'act', 'if', 'they', 'had', 'gun', 'pressed', 'against', 'their', 'foreheads', 'all', 'they', 'do', 'is', 'curse', 'and', 'shoot', 'each', 'other', 'and', 'acting', 'like', 'clichÃ©e', 'version', 'of', 'gangstersbr', 'br', 'the', 'movie', 'doesnt', 'take', 'more', 'than', 'five', 'minutes', 'to', 'explain', 'what', 'is', 'going', 'on', 'before', 'were', 'already', 'at', 'the', 'warehouse', 'there', 'is', 'not', 'single', 'sympathetic', 'character', 'in', 'this', 'movie', 'except', 'for', 'the', 'homeless', 'guy', 'who', 'is', 'also', 'the', 'only', 'one', 'with', 'half', 'brainbr', 'br', 'bill', 'paxton', 'and', 'william', 'sadler', 'are', 'both', 'hill', 'billies', 'and', 'sadlers', 'character', 'is', 'just', 'as', 'much', 'villain', 'as', 'the', 'gangsters', 'didnt', 'like', 'him', 'right', 'from', 'the', 'startbr', 'br', 'the', 'movie', 'is', 'filled', 'with', 'pointless', 'violence', 'and', 'walter', 'hills', 'specialty', 'people', 'falling', 'through', 'windows', 'with', 'glass', 'flying', 'everywhere', 'there', 'is', 'pretty', 'much', 'no', 'plot', 'and', 'it', 'is', 'big', 'problem', 'when', 'you', 'root', 'for', 'noone', 'everybody', 'dies', 'except', 'from', 'paxton', 'and', 'the', 'homeless', 'guy', 'and', 'everybody', 'get', 'what', 'they', 'deservebr', 'br', 'the', 'only', 'two', 'black', 'people', 'that', 'can', 'act', 'is', 'the', 'homeless', 'guy', 'and', 'the', 'junkie', 'but', 'theyre', 'actors', 'by', 'profession', 'not', 'annoying', 'ugly', 'brain', 'dead', 'rappersbr', 'br', 'stay', 'away', 'from', 'this', 'crap', 'and', 'watch', 'hours', 'and', 'instead', 'at', 'lest', 'they', 'have', 'characters', 'you', 'care', 'about', 'sense', 'of', 'humor', 'and', 'nothing', 'but', 'real', 'actors', 'in', 'the', 'cast']),\n",
              "       ...,\n",
              "       list(['i', 'am', 'amazed', 'at', 'how', 'this', 'movieand', 'most', 'others', 'has', 'average', 'stars', 'and', 'lower', 'when', 'there', 'are', 'crappy', 'movies', 'averaging', 'to', 'stars', 'on', 'imdb', 'the', 'fanboy', 'mentality', 'strikes', 'again', 'when', 'this', 'movie', 'came', 'out', 'just', 'about', 'everyone', 'slammed', 'it', 'even', 'my', 'exgirlfriend', 'said', 'this', 'movie', 'questionable', 'years', 'later', 'sat', 'down', 'to', 'watch', 'this', 'movie', 'and', 'found', 'myself', 'enjoying', 'even', 'laughing', 'quite', 'bit', 'this', 'and', 'the', 'replacement', 'killers', 'are', 'the', 'movies', 'that', 'had', 'people', 'labeling', 'the', 'director', 'antoine', 'fuqua', 'as', 'the', 'black', 'michael', 'bay', 'dont', 'see', 'how', 'since', 'most', 'of', 'fuquas', 'movies', 'are', 'smarter', 'than', 'anything', 'michael', 'bay', 'has', 'came', 'up', 'with', 'at', 'any', 'ratebr', 'br', 'story', 'alvin', 'sandersjamie', 'foxx', 'is', 'former', 'convict', 'that', 'is', 'used', 'by', 'nononsense', 'treasury', 'agent', 'edgardavid', 'morse', 'as', 'pawn', 'to', 'catch', 'killer', 'named', 'bristoldoug', 'hutchinson', 'alvins', 'every', 'moves', 'are', 'tracked', 'by', 'bug', 'implanted', 'in', 'his', 'jaw', 'after', 'an', 'accident', 'while', 'these', 'agents', 'are', 'after', 'bristol', 'bristol', 'is', 'after', 'the', 'gold', 'bricks', 'that', 'were', 'taken', 'in', 'heist', 'gone', 'awrybr', 'br', 'jamie', 'foxx', 'is', 'funny', 'as', 'well', 'as', 'great', 'as', 'alvin', 'sanders', 'alvin', 'is', 'fasttalker', 'that', 'is', 'lot', 'smarter', 'than', 'he', 'lets', 'on', 'doug', 'hutchinson', 'is', 'okay', 'as', 'bristol', 'he', 'can', 'be', 'overthetop', 'sometimes', 'in', 'his', 'john', 'malkovitchesque', 'demeanor', 'he', 'was', 'better', 'here', 'than', 'he', 'was', 'as', 'looney', 'bin', 'jim', 'in', 'punisher', 'war', 'zone', 'david', 'morse', 'is', 'good', 'as', 'the', 'hard', 'edged', 'treasury', 'agent', 'even', 'mike', 'epps', 'is', 'funny', 'as', 'alvins', 'brother', 'stevie', 'both', 'him', 'and', 'jamie', 'had', 'some', 'funny', 'moments', 'on', 'screenbr', 'br', 'the', 'only', 'flaw', 'of', 'the', 'movie', 'is', 'the', 'some', 'of', 'the', 'attempts', 'at', 'thriller', 'fall', 'flat', 'the', 'scenario', 'at', 'the', 'horse', 'race', 'track', 'is', 'way', 'overthetop', 'but', 'couldnt', 'look', 'away', 'the', 'director', 'went', 'all', 'out', 'there', 'so', 'he', 'gets', 'points', 'for', 'that', 'plus', 'the', 'bomb', 'scene', 'with', 'the', 'treasury', 'agent', 'tied', 'to', 'chair', 'while', 'the', 'detonator', 'rests', 'on', 'the', 'door', 'was', 'pretty', 'niftybr', 'br', 'all', 'in', 'all', 'bait', 'is', 'not', 'bad', 'movie', 'by', 'long', 'shot', 'its', 'never', 'boring', 'its', 'always', 'funny', 'and', 'wasnt', 'checking', 'my', 'watch', 'every', 'minute', 'that', 'should', 'count', 'for', 'something', 'bait', 'is', 'one', 'of', 'the', 'most', 'underrated', 'movies', 'of', 'periodbr', 'br', 'ps', 'to', 'the', 'reviewer', 'that', 'claimed', 'this', 'movie', 'is', 'too', 'violent', 'how', 'long', 'have', 'you', 'been', 'living', 'under', 'rock', 'im', 'pretty', 'sure', 'youve', 'seen', 'the', 'die', 'hard', 'series', 'and', 'every', 'movie', 'by', 'quentin', 'tarantino', 'but', 'those', 'movies', 'arent', 'violent', 'right', 'weirdo']),\n",
              "       list(['a', 'christmas', 'together', 'actually', 'came', 'before', 'my', 'time', 'but', 'ive', 'been', 'raised', 'on', 'john', 'denver', 'and', 'the', 'songs', 'from', 'this', 'special', 'were', 'always', 'my', 'familys', 'christmas', 'music', 'for', 'years', 'we', 'had', 'crackling', 'cassette', 'made', 'from', 'record', 'that', 'meant', 'it', 'was', 'christmas', 'few', 'years', 'ago', 'was', 'finally', 'able', 'to', 'track', 'down', 'video', 'of', 'it', 'on', 'ebay', 'so', 'after', 'listening', 'to', 'all', 'the', 'music', 'for', 'some', 'years', 'got', 'to', 'see', 'john', 'and', 'the', 'muppets', 'in', 'action', 'for', 'myself', 'if', 'you', 'ever', 'get', 'the', 'chance', 'its', 'lot', 'of', 'fungreat', 'music', 'heartwarming', 'and', 'cheesy', 'its', 'also', 'interesting', 'to', 'see', 'the', 'versions', 'of', 'the', 'muppets', 'and', 'compare', 'them', 'to', 'their', 'newer', 'versions', 'today', 'believe', 'denver', 'actually', 'took', 'some', 'heat', 'for', 'doing', 'show', 'like', 'thisi', 'guess', 'normally', 'performers', 'dont', 'compromise', 'their', 'images', 'by', 'doing', 'singalongs', 'with', 'the', 'muppets', 'but', 'im', 'glad', 'he', 'did', 'even', 'if', 'you', 'cant', 'track', 'down', 'the', 'video', 'the', 'soundtrack', 'is', 'worth', 'it', 'too', 'it', 'has', 'some', 'muppified', 'traditional', 'favorites', 'but', 'also', 'some', 'original', 'denver', 'tunes', 'as', 'well']),\n",
              "       list(['workingclass', 'romantic', 'drama', 'from', 'director', 'martin', 'ritt', 'is', 'as', 'unbelievable', 'as', 'they', 'come', 'yet', 'there', 'are', 'moments', 'of', 'pleasure', 'due', 'mostly', 'to', 'the', 'charisma', 'of', 'stars', 'jane', 'fonda', 'and', 'robert', 'de', 'niro', 'both', 'terrific', 'shes', 'widow', 'who', 'cant', 'move', 'on', 'hes', 'illiterate', 'and', 'closetinventoryou', 'can', 'probably', 'guess', 'the', 'rest', 'adaptation', 'of', 'pat', 'barkers', 'novel', 'union', 'street', 'better', 'title', 'is', 'so', 'laidback', 'it', 'verges', 'on', 'bland', 'and', 'the', 'films', 'editing', 'is', 'mess', 'but', 'its', 'still', 'pleasant', 'rosyhued', 'bluecollar', 'fantasy', 'there', 'are', 'no', 'overtures', 'to', 'serious', 'issues', 'even', 'the', 'illiteracy', 'angle', 'is', 'just', 'plottool', 'for', 'the', 'ensuing', 'love', 'story', 'and', 'no', 'real', 'fireworks', 'though', 'the', 'characters', 'are', 'intentionally', 'bit', 'colorless', 'and', 'the', 'leads', 'are', 'toned', 'down', 'to', 'an', 'interesting', 'degree', 'the', 'finale', 'is', 'pure', 'fluffand', 'cynics', 'will', 'find', 'it', 'difficult', 'to', 'swallowthough', 'these', 'two', 'characters', 'deserve', 'happy', 'ending', 'and', 'the', 'picture', 'wouldnt', 'really', 'be', 'satisfying', 'any', 'other', 'way', 'from'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use a Word Net Lemmatizer to get the lemma of each word so that our feature space is reduced."
      ],
      "metadata": {
        "id": "Zjf_SW65oBqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemma = WordNetLemmatizer() \n",
        "\n",
        "for sent in sentences:\n",
        "  for word in sent:\n",
        "    lemma.lemmatize(word)"
      ],
      "metadata": {
        "id": "mwHigsMawMsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentences after Lemmatization"
      ],
      "metadata": {
        "id": "B9YSi4fGoJIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "5EUUNGuXsZji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b076d4e5-7e4e-4173-d518-607d67b863e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['once', 'again', 'mr', 'costner', 'has', 'dragged', 'out', 'movie', 'for', 'far', 'longer', 'than', 'necessary', 'aside', 'from', 'the', 'terrific', 'sea', 'rescue', 'sequences', 'of', 'which', 'there', 'are', 'very', 'few', 'just', 'did', 'not', 'care', 'about', 'any', 'of', 'the', 'characters', 'most', 'of', 'us', 'have', 'ghosts', 'in', 'the', 'closet', 'and', 'costners', 'character', 'are', 'realized', 'early', 'on', 'and', 'then', 'forgotten', 'until', 'much', 'later', 'by', 'which', 'time', 'did', 'not', 'care', 'the', 'character', 'we', 'should', 'really', 'care', 'about', 'is', 'very', 'cocky', 'overconfident', 'ashton', 'kutcher', 'the', 'problem', 'is', 'he', 'comes', 'off', 'as', 'kid', 'who', 'thinks', 'hes', 'better', 'than', 'anyone', 'else', 'around', 'him', 'and', 'shows', 'no', 'signs', 'of', 'cluttered', 'closet', 'his', 'only', 'obstacle', 'appears', 'to', 'be', 'winning', 'over', 'costner', 'finally', 'when', 'we', 'are', 'well', 'past', 'the', 'half', 'way', 'point', 'of', 'this', 'stinker', 'costner', 'tells', 'us', 'all', 'about', 'kutchers', 'ghosts', 'we', 'are', 'told', 'why', 'kutcher', 'is', 'driven', 'to', 'be', 'the', 'best', 'with', 'no', 'prior', 'inkling', 'or', 'foreshadowing', 'no', 'magic', 'here', 'it', 'was', 'all', 'could', 'do', 'to', 'keep', 'from', 'turning', 'it', 'off', 'an', 'hour', 'in']),\n",
              "       list(['this', 'is', 'an', 'example', 'of', 'why', 'the', 'majority', 'of', 'action', 'films', 'are', 'the', 'same', 'generic', 'and', 'boring', 'theres', 'really', 'nothing', 'worth', 'watching', 'here', 'complete', 'waste', 'of', 'the', 'then', 'barelytapped', 'talents', 'of', 'icet', 'and', 'ice', 'cube', 'whove', 'each', 'proven', 'many', 'times', 'over', 'that', 'they', 'are', 'capable', 'of', 'acting', 'and', 'acting', 'well', 'dont', 'bother', 'with', 'this', 'one', 'go', 'see', 'new', 'jack', 'city', 'ricochet', 'or', 'watch', 'new', 'york', 'undercover', 'for', 'icet', 'or', 'boyz', 'the', 'hood', 'higher', 'learning', 'or', 'friday', 'for', 'ice', 'cube', 'and', 'see', 'the', 'real', 'deal', 'icets', 'horribly', 'cliched', 'dialogue', 'alone', 'makes', 'this', 'film', 'grate', 'at', 'the', 'teeth', 'and', 'im', 'still', 'wondering', 'what', 'the', 'heck', 'bill', 'paxton', 'was', 'doing', 'in', 'this', 'film', 'and', 'why', 'the', 'heck', 'does', 'he', 'always', 'play', 'the', 'exact', 'same', 'character', 'from', 'aliens', 'onward', 'every', 'film', 'ive', 'seen', 'with', 'bill', 'paxton', 'has', 'him', 'playing', 'the', 'exact', 'same', 'irritating', 'character', 'and', 'at', 'least', 'in', 'aliens', 'his', 'character', 'died', 'which', 'made', 'it', 'somewhat', 'gratifyingbr', 'br', 'overall', 'this', 'is', 'secondrate', 'action', 'trash', 'there', 'are', 'countless', 'better', 'films', 'to', 'see', 'and', 'if', 'you', 'really', 'want', 'to', 'see', 'this', 'one', 'watch', 'judgement', 'night', 'which', 'is', 'practically', 'carbon', 'copy', 'but', 'has', 'better', 'acting', 'and', 'better', 'script', 'the', 'only', 'thing', 'that', 'made', 'this', 'at', 'all', 'worth', 'watching', 'was', 'decent', 'hand', 'on', 'the', 'camera', 'the', 'cinematography', 'was', 'almost', 'refreshing', 'which', 'comes', 'close', 'to', 'making', 'up', 'for', 'the', 'horrible', 'film', 'itself', 'but', 'not', 'quite']),\n",
              "       list(['first', 'of', 'all', 'hate', 'those', 'moronic', 'rappers', 'who', 'couldnt', 'act', 'if', 'they', 'had', 'gun', 'pressed', 'against', 'their', 'foreheads', 'all', 'they', 'do', 'is', 'curse', 'and', 'shoot', 'each', 'other', 'and', 'acting', 'like', 'clichÃ©e', 'version', 'of', 'gangstersbr', 'br', 'the', 'movie', 'doesnt', 'take', 'more', 'than', 'five', 'minutes', 'to', 'explain', 'what', 'is', 'going', 'on', 'before', 'were', 'already', 'at', 'the', 'warehouse', 'there', 'is', 'not', 'single', 'sympathetic', 'character', 'in', 'this', 'movie', 'except', 'for', 'the', 'homeless', 'guy', 'who', 'is', 'also', 'the', 'only', 'one', 'with', 'half', 'brainbr', 'br', 'bill', 'paxton', 'and', 'william', 'sadler', 'are', 'both', 'hill', 'billies', 'and', 'sadlers', 'character', 'is', 'just', 'as', 'much', 'villain', 'as', 'the', 'gangsters', 'didnt', 'like', 'him', 'right', 'from', 'the', 'startbr', 'br', 'the', 'movie', 'is', 'filled', 'with', 'pointless', 'violence', 'and', 'walter', 'hills', 'specialty', 'people', 'falling', 'through', 'windows', 'with', 'glass', 'flying', 'everywhere', 'there', 'is', 'pretty', 'much', 'no', 'plot', 'and', 'it', 'is', 'big', 'problem', 'when', 'you', 'root', 'for', 'noone', 'everybody', 'dies', 'except', 'from', 'paxton', 'and', 'the', 'homeless', 'guy', 'and', 'everybody', 'get', 'what', 'they', 'deservebr', 'br', 'the', 'only', 'two', 'black', 'people', 'that', 'can', 'act', 'is', 'the', 'homeless', 'guy', 'and', 'the', 'junkie', 'but', 'theyre', 'actors', 'by', 'profession', 'not', 'annoying', 'ugly', 'brain', 'dead', 'rappersbr', 'br', 'stay', 'away', 'from', 'this', 'crap', 'and', 'watch', 'hours', 'and', 'instead', 'at', 'lest', 'they', 'have', 'characters', 'you', 'care', 'about', 'sense', 'of', 'humor', 'and', 'nothing', 'but', 'real', 'actors', 'in', 'the', 'cast']),\n",
              "       ...,\n",
              "       list(['i', 'am', 'amazed', 'at', 'how', 'this', 'movieand', 'most', 'others', 'has', 'average', 'stars', 'and', 'lower', 'when', 'there', 'are', 'crappy', 'movies', 'averaging', 'to', 'stars', 'on', 'imdb', 'the', 'fanboy', 'mentality', 'strikes', 'again', 'when', 'this', 'movie', 'came', 'out', 'just', 'about', 'everyone', 'slammed', 'it', 'even', 'my', 'exgirlfriend', 'said', 'this', 'movie', 'questionable', 'years', 'later', 'sat', 'down', 'to', 'watch', 'this', 'movie', 'and', 'found', 'myself', 'enjoying', 'even', 'laughing', 'quite', 'bit', 'this', 'and', 'the', 'replacement', 'killers', 'are', 'the', 'movies', 'that', 'had', 'people', 'labeling', 'the', 'director', 'antoine', 'fuqua', 'as', 'the', 'black', 'michael', 'bay', 'dont', 'see', 'how', 'since', 'most', 'of', 'fuquas', 'movies', 'are', 'smarter', 'than', 'anything', 'michael', 'bay', 'has', 'came', 'up', 'with', 'at', 'any', 'ratebr', 'br', 'story', 'alvin', 'sandersjamie', 'foxx', 'is', 'former', 'convict', 'that', 'is', 'used', 'by', 'nononsense', 'treasury', 'agent', 'edgardavid', 'morse', 'as', 'pawn', 'to', 'catch', 'killer', 'named', 'bristoldoug', 'hutchinson', 'alvins', 'every', 'moves', 'are', 'tracked', 'by', 'bug', 'implanted', 'in', 'his', 'jaw', 'after', 'an', 'accident', 'while', 'these', 'agents', 'are', 'after', 'bristol', 'bristol', 'is', 'after', 'the', 'gold', 'bricks', 'that', 'were', 'taken', 'in', 'heist', 'gone', 'awrybr', 'br', 'jamie', 'foxx', 'is', 'funny', 'as', 'well', 'as', 'great', 'as', 'alvin', 'sanders', 'alvin', 'is', 'fasttalker', 'that', 'is', 'lot', 'smarter', 'than', 'he', 'lets', 'on', 'doug', 'hutchinson', 'is', 'okay', 'as', 'bristol', 'he', 'can', 'be', 'overthetop', 'sometimes', 'in', 'his', 'john', 'malkovitchesque', 'demeanor', 'he', 'was', 'better', 'here', 'than', 'he', 'was', 'as', 'looney', 'bin', 'jim', 'in', 'punisher', 'war', 'zone', 'david', 'morse', 'is', 'good', 'as', 'the', 'hard', 'edged', 'treasury', 'agent', 'even', 'mike', 'epps', 'is', 'funny', 'as', 'alvins', 'brother', 'stevie', 'both', 'him', 'and', 'jamie', 'had', 'some', 'funny', 'moments', 'on', 'screenbr', 'br', 'the', 'only', 'flaw', 'of', 'the', 'movie', 'is', 'the', 'some', 'of', 'the', 'attempts', 'at', 'thriller', 'fall', 'flat', 'the', 'scenario', 'at', 'the', 'horse', 'race', 'track', 'is', 'way', 'overthetop', 'but', 'couldnt', 'look', 'away', 'the', 'director', 'went', 'all', 'out', 'there', 'so', 'he', 'gets', 'points', 'for', 'that', 'plus', 'the', 'bomb', 'scene', 'with', 'the', 'treasury', 'agent', 'tied', 'to', 'chair', 'while', 'the', 'detonator', 'rests', 'on', 'the', 'door', 'was', 'pretty', 'niftybr', 'br', 'all', 'in', 'all', 'bait', 'is', 'not', 'bad', 'movie', 'by', 'long', 'shot', 'its', 'never', 'boring', 'its', 'always', 'funny', 'and', 'wasnt', 'checking', 'my', 'watch', 'every', 'minute', 'that', 'should', 'count', 'for', 'something', 'bait', 'is', 'one', 'of', 'the', 'most', 'underrated', 'movies', 'of', 'periodbr', 'br', 'ps', 'to', 'the', 'reviewer', 'that', 'claimed', 'this', 'movie', 'is', 'too', 'violent', 'how', 'long', 'have', 'you', 'been', 'living', 'under', 'rock', 'im', 'pretty', 'sure', 'youve', 'seen', 'the', 'die', 'hard', 'series', 'and', 'every', 'movie', 'by', 'quentin', 'tarantino', 'but', 'those', 'movies', 'arent', 'violent', 'right', 'weirdo']),\n",
              "       list(['a', 'christmas', 'together', 'actually', 'came', 'before', 'my', 'time', 'but', 'ive', 'been', 'raised', 'on', 'john', 'denver', 'and', 'the', 'songs', 'from', 'this', 'special', 'were', 'always', 'my', 'familys', 'christmas', 'music', 'for', 'years', 'we', 'had', 'crackling', 'cassette', 'made', 'from', 'record', 'that', 'meant', 'it', 'was', 'christmas', 'few', 'years', 'ago', 'was', 'finally', 'able', 'to', 'track', 'down', 'video', 'of', 'it', 'on', 'ebay', 'so', 'after', 'listening', 'to', 'all', 'the', 'music', 'for', 'some', 'years', 'got', 'to', 'see', 'john', 'and', 'the', 'muppets', 'in', 'action', 'for', 'myself', 'if', 'you', 'ever', 'get', 'the', 'chance', 'its', 'lot', 'of', 'fungreat', 'music', 'heartwarming', 'and', 'cheesy', 'its', 'also', 'interesting', 'to', 'see', 'the', 'versions', 'of', 'the', 'muppets', 'and', 'compare', 'them', 'to', 'their', 'newer', 'versions', 'today', 'believe', 'denver', 'actually', 'took', 'some', 'heat', 'for', 'doing', 'show', 'like', 'thisi', 'guess', 'normally', 'performers', 'dont', 'compromise', 'their', 'images', 'by', 'doing', 'singalongs', 'with', 'the', 'muppets', 'but', 'im', 'glad', 'he', 'did', 'even', 'if', 'you', 'cant', 'track', 'down', 'the', 'video', 'the', 'soundtrack', 'is', 'worth', 'it', 'too', 'it', 'has', 'some', 'muppified', 'traditional', 'favorites', 'but', 'also', 'some', 'original', 'denver', 'tunes', 'as', 'well']),\n",
              "       list(['workingclass', 'romantic', 'drama', 'from', 'director', 'martin', 'ritt', 'is', 'as', 'unbelievable', 'as', 'they', 'come', 'yet', 'there', 'are', 'moments', 'of', 'pleasure', 'due', 'mostly', 'to', 'the', 'charisma', 'of', 'stars', 'jane', 'fonda', 'and', 'robert', 'de', 'niro', 'both', 'terrific', 'shes', 'widow', 'who', 'cant', 'move', 'on', 'hes', 'illiterate', 'and', 'closetinventoryou', 'can', 'probably', 'guess', 'the', 'rest', 'adaptation', 'of', 'pat', 'barkers', 'novel', 'union', 'street', 'better', 'title', 'is', 'so', 'laidback', 'it', 'verges', 'on', 'bland', 'and', 'the', 'films', 'editing', 'is', 'mess', 'but', 'its', 'still', 'pleasant', 'rosyhued', 'bluecollar', 'fantasy', 'there', 'are', 'no', 'overtures', 'to', 'serious', 'issues', 'even', 'the', 'illiteracy', 'angle', 'is', 'just', 'plottool', 'for', 'the', 'ensuing', 'love', 'story', 'and', 'no', 'real', 'fireworks', 'though', 'the', 'characters', 'are', 'intentionally', 'bit', 'colorless', 'and', 'the', 'leads', 'are', 'toned', 'down', 'to', 'an', 'interesting', 'degree', 'the', 'finale', 'is', 'pure', 'fluffand', 'cynics', 'will', 'find', 'it', 'difficult', 'to', 'swallowthough', 'these', 'two', 'characters', 'deserve', 'happy', 'ending', 'and', 'the', 'picture', 'wouldnt', 'really', 'be', 'satisfying', 'any', 'other', 'way', 'from'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After Lemmatizing, we need to rejoin the tokenized words into a sentence"
      ],
      "metadata": {
        "id": "yzLO0JYcoMJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a temp list to store the joined sentences\n",
        "sent_list = []\n",
        "for sent in sentences:\n",
        "  sent_list.append(' '.join(sent))\n",
        "#Overwrite sentences with the temp array\n",
        "sentences = np.asarray(sent_list)"
      ],
      "metadata": {
        "id": "FO3od79ExBRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what the sentence attribute looks like after preprocessing"
      ],
      "metadata": {
        "id": "U7gy50SSoiUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "id": "yhWyDXHbyHoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2424761-9a8e-4c52-e1b2-62dcdfe3b750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['once again mr costner has dragged out movie for far longer than necessary aside from the terrific sea rescue sequences of which there are very few just did not care about any of the characters most of us have ghosts in the closet and costners character are realized early on and then forgotten until much later by which time did not care the character we should really care about is very cocky overconfident ashton kutcher the problem is he comes off as kid who thinks hes better than anyone else around him and shows no signs of cluttered closet his only obstacle appears to be winning over costner finally when we are well past the half way point of this stinker costner tells us all about kutchers ghosts we are told why kutcher is driven to be the best with no prior inkling or foreshadowing no magic here it was all could do to keep from turning it off an hour in',\n",
              "       'this is an example of why the majority of action films are the same generic and boring theres really nothing worth watching here complete waste of the then barelytapped talents of icet and ice cube whove each proven many times over that they are capable of acting and acting well dont bother with this one go see new jack city ricochet or watch new york undercover for icet or boyz the hood higher learning or friday for ice cube and see the real deal icets horribly cliched dialogue alone makes this film grate at the teeth and im still wondering what the heck bill paxton was doing in this film and why the heck does he always play the exact same character from aliens onward every film ive seen with bill paxton has him playing the exact same irritating character and at least in aliens his character died which made it somewhat gratifyingbr br overall this is secondrate action trash there are countless better films to see and if you really want to see this one watch judgement night which is practically carbon copy but has better acting and better script the only thing that made this at all worth watching was decent hand on the camera the cinematography was almost refreshing which comes close to making up for the horrible film itself but not quite',\n",
              "       'first of all hate those moronic rappers who couldnt act if they had gun pressed against their foreheads all they do is curse and shoot each other and acting like clichÃ©e version of gangstersbr br the movie doesnt take more than five minutes to explain what is going on before were already at the warehouse there is not single sympathetic character in this movie except for the homeless guy who is also the only one with half brainbr br bill paxton and william sadler are both hill billies and sadlers character is just as much villain as the gangsters didnt like him right from the startbr br the movie is filled with pointless violence and walter hills specialty people falling through windows with glass flying everywhere there is pretty much no plot and it is big problem when you root for noone everybody dies except from paxton and the homeless guy and everybody get what they deservebr br the only two black people that can act is the homeless guy and the junkie but theyre actors by profession not annoying ugly brain dead rappersbr br stay away from this crap and watch hours and instead at lest they have characters you care about sense of humor and nothing but real actors in the cast',\n",
              "       ...,\n",
              "       'i am amazed at how this movieand most others has average stars and lower when there are crappy movies averaging to stars on imdb the fanboy mentality strikes again when this movie came out just about everyone slammed it even my exgirlfriend said this movie questionable years later sat down to watch this movie and found myself enjoying even laughing quite bit this and the replacement killers are the movies that had people labeling the director antoine fuqua as the black michael bay dont see how since most of fuquas movies are smarter than anything michael bay has came up with at any ratebr br story alvin sandersjamie foxx is former convict that is used by nononsense treasury agent edgardavid morse as pawn to catch killer named bristoldoug hutchinson alvins every moves are tracked by bug implanted in his jaw after an accident while these agents are after bristol bristol is after the gold bricks that were taken in heist gone awrybr br jamie foxx is funny as well as great as alvin sanders alvin is fasttalker that is lot smarter than he lets on doug hutchinson is okay as bristol he can be overthetop sometimes in his john malkovitchesque demeanor he was better here than he was as looney bin jim in punisher war zone david morse is good as the hard edged treasury agent even mike epps is funny as alvins brother stevie both him and jamie had some funny moments on screenbr br the only flaw of the movie is the some of the attempts at thriller fall flat the scenario at the horse race track is way overthetop but couldnt look away the director went all out there so he gets points for that plus the bomb scene with the treasury agent tied to chair while the detonator rests on the door was pretty niftybr br all in all bait is not bad movie by long shot its never boring its always funny and wasnt checking my watch every minute that should count for something bait is one of the most underrated movies of periodbr br ps to the reviewer that claimed this movie is too violent how long have you been living under rock im pretty sure youve seen the die hard series and every movie by quentin tarantino but those movies arent violent right weirdo',\n",
              "       'a christmas together actually came before my time but ive been raised on john denver and the songs from this special were always my familys christmas music for years we had crackling cassette made from record that meant it was christmas few years ago was finally able to track down video of it on ebay so after listening to all the music for some years got to see john and the muppets in action for myself if you ever get the chance its lot of fungreat music heartwarming and cheesy its also interesting to see the versions of the muppets and compare them to their newer versions today believe denver actually took some heat for doing show like thisi guess normally performers dont compromise their images by doing singalongs with the muppets but im glad he did even if you cant track down the video the soundtrack is worth it too it has some muppified traditional favorites but also some original denver tunes as well',\n",
              "       'workingclass romantic drama from director martin ritt is as unbelievable as they come yet there are moments of pleasure due mostly to the charisma of stars jane fonda and robert de niro both terrific shes widow who cant move on hes illiterate and closetinventoryou can probably guess the rest adaptation of pat barkers novel union street better title is so laidback it verges on bland and the films editing is mess but its still pleasant rosyhued bluecollar fantasy there are no overtures to serious issues even the illiteracy angle is just plottool for the ensuing love story and no real fireworks though the characters are intentionally bit colorless and the leads are toned down to an interesting degree the finale is pure fluffand cynics will find it difficult to swallowthough these two characters deserve happy ending and the picture wouldnt really be satisfying any other way from'],\n",
              "      dtype='<U13141')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing inputs before Neural Network"
      ],
      "metadata": {
        "id": "al5ouKDOptSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now preprocess our labels. Our labels are not enumerated so we can encode them with a Label Encoder so each label has a discrete numerical value (0 or 1)."
      ],
      "metadata": {
        "id": "3_128pM0omMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "a6lsQ9rDd-1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The keras Tokenizer takes each sentence and transforms it into a numerical representation of that sentence"
      ],
      "metadata": {
        "id": "Qpis-K5royfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = 2000)\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "metadata": {
        "id": "1N0Exw_nb7rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to capture input lengths for the NN and further transform our input."
      ],
      "metadata": {
        "id": "omQToOBOpSzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a max length so we can define our NN input\n",
        "max_review_len = max([len(s.split()) for s in sentences])\n",
        "\n",
        "#vocab size is the number of distinct words in our sentence input. Ideally we want to minimize this variable (which we do through pre-processing)\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "\n",
        "# Transforms each text in texts to a sequence of integers. \n",
        "sentences = tokenizer.texts_to_sequences(sentences)   \n",
        "\n",
        "# Add extra characters to each sentence so they are uniform length\n",
        "padded_docs = pad_sequences(sentences,maxlen=max_review_len)"
      ],
      "metadata": {
        "id": "H2-jOZ0QcUS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split our input into training and testing subsets"
      ],
      "metadata": {
        "id": "eKaLuf6ipcJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_docs, y, test_size=0.30, random_state=42)"
      ],
      "metadata": {
        "id": "vr_5h4TAhVOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Callbacks for overfitting"
      ],
      "metadata": {
        "id": "O6LDQMN1V55P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce Learning Rate callback"
      ],
      "metadata": {
        "id": "1-NgJL91qJBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "  monitor='val_loss', \n",
        "  factor=0.2,\n",
        "  patience=5, \n",
        "  min_lr=0.001\n",
        ")"
      ],
      "metadata": {
        "id": "ns5KlBDyQCEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early Stopping callback"
      ],
      "metadata": {
        "id": "2jMVAxkmqNFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earlyStop = keras.callbacks.EarlyStopping(\n",
        "  monitor='val_loss',\n",
        "  patience=5, \n",
        "  verbose=1,\n",
        "  mode='auto', \n",
        "  restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "4TfmwzBtqIEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Embedded Model\n",
        "\n",
        "Accuracy: 86%"
      ],
      "metadata": {
        "id": "BW2rEbZuVzWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embedded_model():\n",
        "  model = Sequential()\n",
        "  #added embedding layer\n",
        "  model.add(Embedding(vocab_size, 50, input_length=max_review_len))\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  #model.add(GlobalMaxPool1D())\n",
        "  model.add(Dense(300, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  #Compile model\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "-ubV8pw5dDKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = embedded_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "t-wndsy_oJf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "849a0b40-222e-41de-b375-353b6526b8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 2370, 50)          8893950   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 2363, 32)          12832     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 1181, 32)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 37792)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 300)               11337900  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,244,983\n",
            "Trainable params: 20,244,983\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,y_train, epochs=10, verbose=True, validation_data=(X_test,y_test), batch_size=256, callbacks=[reduce_lr, earlyStop])"
      ],
      "metadata": {
        "id": "oqLhmeNEewN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ab1a47-10b4-4737-d12e-1f2ae9f6a22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "137/137 [==============================] - 324s 2s/step - loss: 0.5107 - accuracy: 0.7211 - val_loss: 0.3263 - val_accuracy: 0.8591 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 334s 2s/step - loss: 0.2827 - accuracy: 0.8824 - val_loss: 0.2844 - val_accuracy: 0.8801 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 340s 2s/step - loss: 0.2481 - accuracy: 0.8977 - val_loss: 0.2788 - val_accuracy: 0.8841 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 329s 2s/step - loss: 0.2187 - accuracy: 0.9124 - val_loss: 0.2821 - val_accuracy: 0.8837 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 327s 2s/step - loss: 0.1814 - accuracy: 0.9323 - val_loss: 0.2967 - val_accuracy: 0.8777 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 329s 2s/step - loss: 0.1279 - accuracy: 0.9560 - val_loss: 0.3370 - val_accuracy: 0.8741 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 333s 2s/step - loss: 0.0640 - accuracy: 0.9845 - val_loss: 0.4180 - val_accuracy: 0.8715 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9959Restoring model weights from the end of the best epoch: 3.\n",
            "137/137 [==============================] - 333s 2s/step - loss: 0.0248 - accuracy: 0.9959 - val_loss: 0.5008 - val_accuracy: 0.8699 - lr: 0.0010\n",
            "Epoch 8: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Embedded Model Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "id": "tgajbtI-An0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ea3c6d-fc23-4d13-9590-901bdcc8a743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded Model Accuracy: 88.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline model\n",
        "\n",
        "Accuracy: 50%"
      ],
      "metadata": {
        "id": "D3ILNkvHVlsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(300, input_dim=max_review_len, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model "
      ],
      "metadata": {
        "id": "ViCsMoiBFkTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = baseline_model()\n",
        "history = model.fit(X_train,y_train, epochs=10, verbose=True, validation_data=(X_test,y_test), batch_size=256,callbacks=[reduce_lr, earlyStop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpWZoM9zMZHI",
        "outputId": "c8884918-9fda-43f6-9a68-32a75da3050c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "137/137 [==============================] - 4s 29ms/step - loss: 2.8142 - accuracy: 0.4952 - val_loss: 0.7052 - val_accuracy: 0.4960 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 4s 27ms/step - loss: 0.6975 - accuracy: 0.5061 - val_loss: 0.6994 - val_accuracy: 0.5039 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 4s 27ms/step - loss: 0.6917 - accuracy: 0.5138 - val_loss: 0.7021 - val_accuracy: 0.5023 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 4s 28ms/step - loss: 0.6882 - accuracy: 0.5175 - val_loss: 0.7045 - val_accuracy: 0.5056 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 4s 28ms/step - loss: 0.6857 - accuracy: 0.5250 - val_loss: 0.7050 - val_accuracy: 0.5066 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 4s 28ms/step - loss: 0.6829 - accuracy: 0.5312 - val_loss: 0.7106 - val_accuracy: 0.5025 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "136/137 [============================>.] - ETA: 0s - loss: 0.6798 - accuracy: 0.5374Restoring model weights from the end of the best epoch: 2.\n",
            "137/137 [==============================] - 4s 28ms/step - loss: 0.6798 - accuracy: 0.5373 - val_loss: 0.7076 - val_accuracy: 0.5119 - lr: 0.0010\n",
            "Epoch 7: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Base Model Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tWBOLtYMea1",
        "outputId": "4108908a-d9b6-4256-d72a-f23e32aa7a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Model Accuracy: 50.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Model on 20_newsgroups\n",
        "\n",
        "Accuracy: 47%"
      ],
      "metadata": {
        "id": "cv6peVXXN80K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "df = fetch_20newsgroups(subset='all',\n",
        "                             shuffle=False, remove=('headers', 'footers', 'quotes')) ##remove unneccessary information"
      ],
      "metadata": {
        "id": "ASEuEEZcYpHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df.data ##will undergo preprocessing to make our padded_doc for test train split\n",
        "y = df.target ##df target is our labels "
      ],
      "metadata": {
        "id": "syTnL3ZvboYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the target shape\n",
        "df.target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X53Q_lO21t3",
        "outputId": "f13ef32f-ef7f-4905-dccb-fa7437aa8014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18846,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#show sentences type, we'll have to turn this into a np.array later\n",
        "print(type(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr3fS2LzyvVb",
        "outputId": "67c33f9d-acb0-486e-8feb-9d05c29ecf3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing"
      ],
      "metadata": {
        "id": "ZJOhwb-1cBUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize a temp list\n",
        "temp = []\n",
        "for i in range(len(sentences)):\n",
        "  #Iterate through the list and filter the strings\n",
        "  sentences[i]=sentences[i].translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "  sentences[i]=''.join(i for i in sentences[i] if not i.isdigit())\n",
        "  sentences[i]=re.sub(r'\\s+',' ',sentences[i],flags=re.I)\n",
        "  sentences[i]=re.sub(r'[!@#$%^&*()_+|\\}{;:/><.}]','',sentences[i],flags=re.I) ##remove punctuation characters\n",
        "  sentences[i]=re.sub(r'\\s+[a-zA-Z]\\s+', ' ',sentences[i])\n",
        "  sentences[i]=re.sub(r\"[a-zA-Z]\", lambda x :  x.group(0).lower(), sentences[i]) ##lower case words\n",
        "  temp.append(sentences[i])\n",
        "sentences = temp ##set sentences to be equal to temp"
      ],
      "metadata": {
        "id": "wu22LxECZiEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenize for each string in the dataset\n",
        "sentences = np.array([word_tokenize(x) for x in sentences])"
      ],
      "metadata": {
        "id": "oCtJpI_PYtNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90849fbc-d90b-4cfd-e0d7-01709071b209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Lemmatize each word via two for loops\n",
        "lemma = WordNetLemmatizer() \n",
        "\n",
        "for sent in sentences:\n",
        "  for word in sent:\n",
        "    lemma.lemmatize(word)"
      ],
      "metadata": {
        "id": "3RTWfCeNyJPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a temp list to store the joined sentences\n",
        "sent_list = []\n",
        "for sent in sentences:\n",
        "  sent_list.append(' '.join(sent))\n",
        "#Overwrite sentences with the temp array\n",
        "sentences = np.asarray(sent_list)"
      ],
      "metadata": {
        "id": "ko_GMsnaxk0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding the target Column to be ints\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "QRzWCVgAZQ87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Updates internal vocabulary based on a list of texts.  \n",
        "tokenizer = Tokenizer(num_words = 2000)\n",
        "tokenizer.fit_on_texts(sentences) #This method creates the vocabulary index based on word frequency."
      ],
      "metadata": {
        "id": "7Biwv20YuorU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepping data for embedding layer\n",
        "max_news_len = max([len(s.split()) for s in sentences])\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "X = tokenizer.texts_to_sequences(sentences) #Transforms each text in texts to a sequence of integers\n",
        "padded_docs = pad_sequences(X, maxlen = max_news_len)"
      ],
      "metadata": {
        "id": "NoI4F4E8uwGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test train split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_docs, y, test_size=0.30, random_state=42)"
      ],
      "metadata": {
        "id": "ulM-vbDmvpby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Embedded Model\n",
        "\n",
        "Accuracy: 47%"
      ],
      "metadata": {
        "id": "b1J98ZTVclHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create our model\n",
        "def embedded_model(vocab_size, max_len):\n",
        "  model = Sequential()\n",
        "  #added embedding layer\n",
        "  model.add(Embedding(vocab_size, 50, input_length=max_len)) \n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  #model.add(GlobalMaxPool1D())\n",
        "  model.add(Dense(300, activation='relu'))\n",
        "  model.add(Dense(20, activation='softmax'))\n",
        "  #Compile model\n",
        "  model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "x88UG42WvUpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize model\n",
        "model = embedded_model(vocab_size, max_news_len)\n",
        "model.summary() #show summary"
      ],
      "metadata": {
        "id": "Wq0zZe8cvfjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d3637d-e610-48be-8146-389f78fdf581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 11092, 50)         6074650   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 11085, 32)         12832     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 5542, 32)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 177344)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 300)               53203500  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                6020      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 59,297,002\n",
            "Trainable params: 59,297,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model with reduce_lr and earlyStop callbacks\n",
        "history = model.fit(X_train,y_train, epochs=10, verbose=True, validation_data=(X_test,y_test), batch_size=256, callbacks=[reduce_lr, earlyStop])"
      ],
      "metadata": {
        "id": "GHcdVTM9vwv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b901b4-19df-463e-9d1e-6fe237f9f8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "52/52 [==============================] - 86s 2s/step - loss: 3.0721 - accuracy: 0.0648 - val_loss: 2.9056 - val_accuracy: 0.0904 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "52/52 [==============================] - 85s 2s/step - loss: 2.7205 - accuracy: 0.1289 - val_loss: 2.5042 - val_accuracy: 0.1682 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "52/52 [==============================] - 83s 2s/step - loss: 2.2605 - accuracy: 0.2445 - val_loss: 2.2234 - val_accuracy: 0.2557 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "52/52 [==============================] - 84s 2s/step - loss: 1.9254 - accuracy: 0.3460 - val_loss: 2.0443 - val_accuracy: 0.3136 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "52/52 [==============================] - 84s 2s/step - loss: 1.6806 - accuracy: 0.4344 - val_loss: 1.9372 - val_accuracy: 0.3803 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "52/52 [==============================] - 83s 2s/step - loss: 1.5010 - accuracy: 0.4974 - val_loss: 1.9028 - val_accuracy: 0.4079 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "52/52 [==============================] - 84s 2s/step - loss: 1.3376 - accuracy: 0.5616 - val_loss: 1.8461 - val_accuracy: 0.4432 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "52/52 [==============================] - 82s 2s/step - loss: 1.2020 - accuracy: 0.6142 - val_loss: 1.8561 - val_accuracy: 0.4613 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "52/52 [==============================] - 83s 2s/step - loss: 1.0923 - accuracy: 0.6479 - val_loss: 1.8450 - val_accuracy: 0.4669 - lr: 0.0010\n",
            "Epoch 10/10\n",
            "52/52 [==============================] - 86s 2s/step - loss: 1.0160 - accuracy: 0.6806 - val_loss: 1.8971 - val_accuracy: 0.4763 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Embedded Model Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "metadata": {
        "id": "ZlkG3VCnvzas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "274b3a2b-6903-4553-c05c-ed045aac37ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded Model Accuracy: 47.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xNdvHCwF3vqW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}